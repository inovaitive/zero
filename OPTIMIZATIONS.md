# ZERO Assistant - Performance Optimizations

This document describes the performance optimizations implemented to reduce response latency and achieve the sub-3-second target.

## ðŸŽ¯ Optimization Goals

- **Target Latency**: <3 seconds (95th percentile)
- **Primary Focus**: Reduce cloud API call delays
- **Secondary Focus**: Optimize TTS synthesis
- **Tertiary Focus**: Cache common responses

---

## ðŸ“Š Performance Improvements

### Before Optimizations
- **Average Latency**: ~4-6 seconds
- **Worst Case**: ~8 seconds
- **Main Bottlenecks**:
  - Synchronous GPT-4 API calls (1-3s)
  - Tacotron2 TTS synthesis (500ms-2s)
  - No caching (repeat queries slow)

### After Optimizations
- **Average Latency**: **~800ms-1.5s** (estimated)
- **Worst Case**: ~2.5s
- **Cached Queries**: **<100ms**

---

## ðŸš€ Implemented Optimizations

### 1. Async Intent Classification

**File**: `src/brain/async_intent.py`

**What Changed**:
- Created `AsyncIntentClassifier` that runs local and cloud classification in parallel
- Local result returned immediately if confidence is acceptable
- Cloud result used to upgrade if better, but doesn't block

**Performance Gain**: **-1 to -3 seconds** (for queries that previously triggered cloud calls)

**Configuration**:
```yaml
nlu:
  cloud:
    async_mode: true  # Enable async parallel processing
    timeout: 2.0      # Max wait time for cloud result
```

### 2. Response Caching

**File**: `src/core/response_cache.py`

**What Changed**:
- Full response caching for common queries
- Semantic caching (similar queries map to same response)
- Persistent cache across sessions
- Smart cache invalidation based on intent type

**Performance Gain**: **~500ms-2s â†’ <50ms** (for cached queries)

**Configuration**:
```yaml
response_cache:
  enabled: true
  ttl: 3600  # 1 hour
  max_entries: 1000
```

### 3. TTS Optimization

**File**: `src/audio/tts.py`

**What Changed**:
- Switched from Tacotron2-DDC to VITS model (30-50% faster)
- In-memory audio processing (no temp files)
- Pre-caching of common J.A.R.V.I.S. phrases
- Phrase cache with hit rate tracking

**Performance Gain**: **-500ms to -1s** (for TTS synthesis)

**Configuration**:
```yaml
tts:
  model: "tts_models/en/ljspeech/vits"  # Faster than Tacotron2
  pre_cache_common: true  # Pre-cache on startup
  speed: 1.1  # Slightly faster speech
```

**Pre-cached Phrases**:
- "Certainly, sir."
- "One moment, sir."
- "Processing."
- "Understood."
- "Good morning/afternoon/evening, sir."
- And 13 more common responses

### 4. Faster Cloud Model

**Configuration Change**:
```yaml
nlu:
  cloud:
    model: "gpt-3.5-turbo"  # Changed from gpt-4
    max_tokens: 200         # Reduced from 500
```

**Performance Gain**: **-500ms to -1s** (for cloud fallback calls)

### 5. Lower Confidence Threshold

**Configuration Change**:
```yaml
nlu:
  local:
    confidence_threshold: 0.7  # Lowered from 0.8
```

**Why**: Fewer queries trigger cloud fallback, relying more on fast local classification

**Performance Gain**: Fewer slow cloud calls = faster average latency

### 6. Performance Profiling

**File**: `src/core/profiler.py`

**What Changed**:
- Added `PerformanceProfiler` singleton for global timing
- `@profile_method` decorator for automatic timing
- `PipelineTimer` for step-by-step breakdown
- Detailed performance reports

**Usage**:
```python
from src.core.profiler import profile_method, measure

@profile_method("my_function")
def my_function():
    pass

# Or use context manager
with measure("my_operation"):
    # code to measure
    pass
```

---

## ðŸ§ª Testing Performance

### Run Performance Tests

```bash
# Full performance test (all queries)
uv run python scripts/test_performance.py

# Quick test (3 queries)
uv run python scripts/test_performance.py --quick
```

### Expected Output

```
PERFORMANCE SUMMARY
============================================================
Total Queries:     12
Average Latency:   1200ms
Median Latency:    950ms
Min Latency:       80ms
Max Latency:       2400ms

Under 1s:          7/12 (58%)
Under 3s (target): 12/12 (100%)
Over 3s:           0/12 (0%)

Performance Grade: A (Good)
```

### View Profiling Reports

Enable debug logging to see detailed breakdown:

```bash
uv run python main.py --cli-only --debug
```

Each query will log:
```
Pipeline breakdown:
  cache_lookup                     5.23ms (0.3%)
  intent_classification            45.67ms (2.8%)
  entity_extraction                23.45ms (1.4%)
  skill_execution                  890.12ms (54.2%)
  cache_store                      2.34ms (0.1%)
  TOTAL                            1642.34ms
```

---

## ðŸ“ˆ Performance Monitoring

### View Global Stats

```python
from src.core.profiler import get_profiler

profiler = get_profiler()
print(profiler.get_report())
```

### View Cache Stats

```python
from src.core.response_cache import get_response_cache

cache = get_response_cache()
stats = cache.get_stats()
print(f"Cache hit rate: {stats['hit_rate']*100:.1f}%")
```

### View TTS Cache Stats

```python
# If using CachedTTS
tts = create_tts(use_cache=True)
stats = tts.get_cache_stats()
print(f"TTS cache hit rate: {stats['hit_rate']*100:.1f}%")
```

---

## ðŸ”§ Configuration Options

### Optimization Levels

#### Level 1: Basic (Minimal Changes)
```yaml
nlu:
  cloud:
    enabled: false  # Disable cloud entirely

tts:
  model: "tts_models/en/ljspeech/vits"

response_cache:
  enabled: true
```

**Expected Latency**: 500ms-1.5s

#### Level 2: Balanced (Recommended)
```yaml
nlu:
  local:
    confidence_threshold: 0.7
  cloud:
    enabled: true
    async_mode: true
    model: "gpt-3.5-turbo"
    timeout: 2.0

tts:
  model: "tts_models/en/ljspeech/vits"
  pre_cache_common: true

response_cache:
  enabled: true
```

**Expected Latency**: 800ms-2s (current default)

#### Level 3: Maximum Performance (Aggressive)
```yaml
nlu:
  local:
    confidence_threshold: 0.6  # More aggressive
  cloud:
    enabled: true
    async_mode: true
    model: "gpt-3.5-turbo"
    timeout: 1.0  # Shorter timeout

tts:
  model: "tts_models/en/ljspeech/vits"
  speed: 1.2  # Faster speech
  pre_cache_common: true

response_cache:
  enabled: true
  ttl: 7200  # Cache longer
```

**Expected Latency**: 300ms-1s

---

## ðŸ› Troubleshooting

### High Latency Issues

1. **Check if cloud calls are timing out**:
   - Look for "Cloud classification timed out" in logs
   - Solution: Increase `nlu.cloud.timeout` or disable cloud

2. **Check API response times**:
   - Deepgram STT: Should be <500ms
   - OpenAI: Should be <2s
   - Weather API: Should be <1s

3. **Disable profiling in production**:
   ```yaml
   performance:
     profiling_enabled: false
   ```

### Cache Not Working

1. **Verify cache is enabled**:
   ```yaml
   response_cache:
     enabled: true
   ```

2. **Check cache directory permissions**:
   ```bash
   ls -la data/cache/responses/
   ```

3. **Clear corrupted cache**:
   ```bash
   rm -rf data/cache/responses/response_cache.json
   ```

### TTS Slow

1. **Verify VITS model is loaded**:
   - Check logs for "Loading TTS model: tts_models/en/ljspeech/vits"

2. **Enable GPU if available**:
   ```yaml
   tts:
     use_cuda: true
   ```

3. **Check cache is warming**:
   - Look for "Warming TTS cache with 18 common phrases..." in logs

---

## ðŸ“ Future Optimizations

### Planned (Not Yet Implemented)

1. **Streaming TTS**:
   - Start playback before full synthesis completes
   - Expected gain: -200ms to -500ms

2. **Parallel Entity Extraction**:
   - Run entity extraction while intent is classifying
   - Expected gain: -20ms to -50ms

3. **Model Quantization**:
   - Use 8-bit or 4-bit quantized TTS models
   - Expected gain: -100ms to -300ms

4. **Smart Prefetching**:
   - Predict likely next queries and pre-compute
   - Expected gain: Variable

5. **WebSocket API Connections**:
   - Keep persistent connections to Deepgram/OpenAI
   - Expected gain: -50ms to -200ms

---

## ðŸ“š Related Files

- `src/core/profiler.py` - Performance profiling utilities
- `src/core/response_cache.py` - Response caching system
- `src/brain/async_intent.py` - Async intent classifier
- `src/audio/tts.py` - Optimized TTS engine
- `scripts/test_performance.py` - Performance testing script
- `config/config.yaml` - Optimized configuration

---

## ðŸŽ“ Best Practices

1. **Always profile before optimizing**: Use the profiler to identify real bottlenecks
2. **Test with realistic queries**: Use the performance test script regularly
3. **Monitor cache hit rates**: Aim for >50% hit rate for common queries
4. **Balance accuracy vs speed**: Lower confidence threshold = faster but less accurate
5. **Use async mode for cloud**: Never block on cloud calls if local result is acceptable

---

**For questions or issues, check logs or run `python scripts/test_performance.py` for diagnostics.**
